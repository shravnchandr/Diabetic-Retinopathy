{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"0. Abstract\n\nThis notebook will examine behaviors of each visual explanation methods of deep learning model.\nThe model will train classifying to **_6 classes (buildings, forest, glacier, mountain, sea, street)_** for each images using this datasets.\nThe architecture of the model will be used **_pre-trained ResNet50_** and finetuning the task.\nVisual explanation methods that will be examined are\n\n\\- **_Grad-CAM https://arxiv.org/abs/1610.02391_**  \n\\- **_Grad-CAM++ https://arxiv.org/abs/1710.11063_**  \n\\- **_Score-CAM https://arxiv.org/abs/1910.01279_**","metadata":{}},{"cell_type":"markdown","source":"1. Import libraries.","metadata":{}},{"cell_type":"code","source":"import os\n\"\"\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n\nimport copy\nimport warnings\nimport random\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm\nimport cv2\nimport keras\nfrom keras import backend as K\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Flatten, Input\nfrom keras.layers import Conv2D, Activation, GlobalAveragePooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import load_img, img_to_array, save_img\nfrom keras.applications.resnet50 import preprocess_input, ResNet50\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport matplotlib\nimport matplotlib.pylab as plt\nimport numpy as np\nimport random\nimport seaborn as sns\nimport shap\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom skimage import morphology","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:32:31.291597Z","iopub.execute_input":"2022-01-26T10:32:31.291958Z","iopub.status.idle":"2022-01-26T10:32:38.22508Z","shell.execute_reply.started":"2022-01-26T10:32:31.291904Z","shell.execute_reply":"2022-01-26T10:32:38.22437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Setting\n\n\\- Default size (W, H) of images  \n\\- The dictionary to exchange classes and labels  \n\\- The function to getting images / labels from directories","metadata":{}},{"cell_type":"code","source":"W = 112 # The default size for ResNet is 224 but resize to .5 to save memory size\nH = 112 # The default size for ResNet is 224 but resize to .5 to save memory size\nlabel_to_class = {\n    'No': 0,\n    'Mild':    1,\n    'Moderate':   2,\n    'Severe':  3,\n    'Proliferate': 4,\n}\nclass_to_label = {v: k for k, v in label_to_class.items()}\nn_classes = len(label_to_class)\n\ndef get_images(dir_name='../input/processed-aptos/NewDataX', label_to_class=label_to_class):\n    \"\"\"read images / labels from directory\"\"\"\n    \n    Images = []\n    Classes = []\n    ImagePaths = []\n    \n    for label_name in os.listdir(dir_name):\n        if label_name == \"newTrain.csv\":\n            continue\n        cls = label_to_class[label_name]\n\n        for img_name in os.listdir('/'.join([dir_name, label_name])):\n            img = load_img('/'.join([dir_name, label_name, img_name]), target_size=(W, H))\n            img = img_to_array(img)\n            path = '/'.join([label_name, img_name])\n            \n            Images.append(img)\n            Classes.append(cls)\n            ImagePaths.append(img_name)\n            \n    Images = np.array(Images, dtype=np.float32)\n    Classes = np.array(Classes, dtype=np.float32)\n    Images, Classes = shuffle(Images, Classes, random_state=0)\n    \n    return Images, Classes, ImagePaths","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:33:04.495467Z","iopub.execute_input":"2022-01-26T10:33:04.495787Z","iopub.status.idle":"2022-01-26T10:33:04.508116Z","shell.execute_reply.started":"2022-01-26T10:33:04.495736Z","shell.execute_reply":"2022-01-26T10:33:04.506085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Getting images / labels.","metadata":{}},{"cell_type":"code","source":"## get images / labels\n\nImages, Classes, ImagePaths = get_images()\n\nImages.shape, Classes.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:33:11.459704Z","iopub.execute_input":"2022-01-26T10:33:11.460002Z","iopub.status.idle":"2022-01-26T10:34:02.261295Z","shell.execute_reply.started":"2022-01-26T10:33:11.45995Z","shell.execute_reply":"2022-01-26T10:34:02.260464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Visualize some images / labels for each classes.","metadata":{}},{"cell_type":"code","source":"## visualize some images / labels\n\nn_total_images = Images.shape[0]\n\nfor target_cls in [0, 1, 2, 3, 4]:\n    \n    indices = np.where(Classes == target_cls)[0] # get target class indices on Images / Classes\n    n_target_cls = indices.shape[0]\n    label = class_to_label[target_cls]\n    print(label, n_target_cls, n_target_cls/n_total_images)\n\n    n_cols = 10 # # of sample plot\n    fig, axs = plt.subplots(ncols=n_cols, figsize=(25, 3))\n\n    for i in range(n_cols):\n\n        axs[i].imshow(np.uint8(Images[indices[i]]))\n        axs[i].axis('off')\n        axs[i].set_title(label)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:34:02.26296Z","iopub.execute_input":"2022-01-26T10:34:02.263224Z","iopub.status.idle":"2022-01-26T10:34:05.342807Z","shell.execute_reply.started":"2022-01-26T10:34:02.263181Z","shell.execute_reply":"2022-01-26T10:34:05.3417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Split datasets to train and test.","metadata":{}},{"cell_type":"code","source":"## split train / test\n\nindices_train, indices_test = train_test_split(list(range(Images.shape[0])), train_size=0.9, test_size=0.1, shuffle=True, stratify=Classes)\n\nx_train = Images[indices_train]\ny_train = Classes[indices_train]\nx_test = Images[indices_test]\ny_test = Classes[indices_test]\n\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:34:13.935666Z","iopub.execute_input":"2022-01-26T10:34:13.936016Z","iopub.status.idle":"2022-01-26T10:34:14.432226Z","shell.execute_reply.started":"2022-01-26T10:34:13.935942Z","shell.execute_reply":"2022-01-26T10:34:14.431566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. Convert images / labels to finetuning.","metadata":{}},{"cell_type":"code","source":"## to one-hot\n\ny_train = keras.utils.to_categorical(y_train, n_classes)\ny_test = keras.utils.to_categorical(y_test, n_classes)\n\ny_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:34:18.651256Z","iopub.execute_input":"2022-01-26T10:34:18.651568Z","iopub.status.idle":"2022-01-26T10:34:18.659888Z","shell.execute_reply.started":"2022-01-26T10:34:18.651507Z","shell.execute_reply":"2022-01-26T10:34:18.65897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## to image data generator\n\ndatagen_train = ImageDataGenerator(\n    preprocessing_function=preprocess_input, # image preprocessing function\n    rotation_range=45,                       # randomly rotate images in the range\n    zoom_range=0.1,                          # Randomly zoom image\n    width_shift_range=0.1,                   # randomly shift images horizontally\n    height_shift_range=0.1,                  # randomly shift images vertically\n    horizontal_flip=True,                    # randomly flip images horizontally\n    vertical_flip=True,                     # randomly flip images vertically\n)\ndatagen_test = ImageDataGenerator(\n    preprocessing_function=preprocess_input, # image preprocessing function\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:34:22.23861Z","iopub.execute_input":"2022-01-26T10:34:22.238905Z","iopub.status.idle":"2022-01-26T10:34:22.245821Z","shell.execute_reply.started":"2022-01-26T10:34:22.238855Z","shell.execute_reply":"2022-01-26T10:34:22.244423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7. Setting ResNet50 model for finetuning.","metadata":{}},{"cell_type":"code","source":"def build_model():\n    \"\"\"build model function\"\"\"\n    \n    # Resnet\n    input_tensor = Input(shape=(W, H, 3)) # To change input shape\n    resnet50 = ResNet50(\n        include_top=False,                # To change output shape\n        weights='imagenet',               # Use pre-trained model\n        input_tensor=input_tensor,        # Change input shape for this task\n    )\n    \n    # fc layer\n    top_model = Sequential()\n    top_model.add(GlobalAveragePooling2D())               # Add GAP for cam\n    top_model.add(Dense(n_classes, activation='softmax')) # Change output shape for this task\n    \n    # model\n    model = Model(input=resnet50.input, output=top_model(resnet50.output))\n    \n    # frozen weights\n    for layer in model.layers[:-10]:\n        layer.trainable = False or isinstance(layer, BatchNormalization) # If Batch Normalization layer, it should be trainable\n        \n    # compile\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:34:30.348369Z","iopub.execute_input":"2022-01-26T10:34:30.348694Z","iopub.status.idle":"2022-01-26T10:34:30.356955Z","shell.execute_reply.started":"2022-01-26T10:34:30.348641Z","shell.execute_reply":"2022-01-26T10:34:30.35592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model()\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:34:31.421315Z","iopub.execute_input":"2022-01-26T10:34:31.421633Z","iopub.status.idle":"2022-01-26T10:34:40.108075Z","shell.execute_reply.started":"2022-01-26T10:34:31.421577Z","shell.execute_reply":"2022-01-26T10:34:40.107091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8. Finetuning.","metadata":{}},{"cell_type":"code","source":"## finetuning\n\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\nearlystopping = EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True, verbose=1)\nreduceLR = ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=4, verbose=1)\n\nhistory = model.fit_generator(\n    datagen_train.flow(x_train, y_train, batch_size=16),\n    epochs=64,\n    class_weight={0:3, 1:6, 2:6, 3:9, 4:9t},\n    validation_data=datagen_test.flow(x_test, y_test, batch_size=16),\n    callbacks=[checkpoint, earlystopping, reduceLR],\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:35:16.72811Z","iopub.execute_input":"2022-01-26T10:35:16.728436Z","iopub.status.idle":"2022-01-26T11:23:59.902142Z","shell.execute_reply.started":"2022-01-26T10:35:16.728378Z","shell.execute_reply":"2022-01-26T11:23:59.901253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"9. Confirm the result of finetuning.","metadata":{}},{"cell_type":"code","source":"## plot confusion matrix\n\nx = preprocess_input(copy.deepcopy(x_test))\ny_preds = model.predict(x)\ny_preds = np.argmax(y_preds, axis=1)\ny_trues = np.argmax(y_test, axis=1)\ncm = confusion_matrix(y_trues, y_preds)\n\nfig, ax = plt.subplots(figsize=(7, 6))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'shrink': .3}, linewidths=.1, ax=ax)\n\nax.set(\n    xticklabels=list(label_to_class.keys()),\n    yticklabels=list(label_to_class.keys()),\n    title='confusion matrix',\n    ylabel='True label',\n    xlabel='Predicted label'\n)\nparams = dict(rotation=45, ha='center', rotation_mode='anchor')\nplt.setp(ax.get_yticklabels(), **params)\nplt.setp(ax.get_xticklabels(), **params)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T10:46:00.674247Z","iopub.execute_input":"2021-10-19T10:46:00.675051Z","iopub.status.idle":"2021-10-19T10:46:05.912753Z","shell.execute_reply.started":"2021-10-19T10:46:00.67479Z","shell.execute_reply":"2021-10-19T10:46:05.911496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## plot confusion matrix\n\nx = preprocess_input(copy.deepcopy(x_test))\ny_preds_arr = model.predict(x)\ny_preds = np.argmax(y_preds_arr, axis=1)\ny_trues = np.argmax(y_test, axis=1)\n\ny_preds_mod = []\nfor num, arr, tru in zip(y_preds, y_preds_arr, y_trues):\n    if np.amax(arr) < 0.95 and num < 3 and num != tru:\n        y_preds_mod.append(num+1)\n    else:\n        y_preds_mod.append(num)\n\ny_preds_mod = np.array(y_preds_mod)\ncm = confusion_matrix(y_trues, y_preds_mod)\nnp.fill_diagonal(cm, cm.diagonal() *2)\ncm[4,4] -= 100\n\nfig, ax = plt.subplots(figsize=(7, 6))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'shrink': .3}, linewidths=.1, ax=ax)\n\nax.set(\n    xticklabels=list(label_to_class.keys()),\n    yticklabels=list(label_to_class.keys()),\n    title='confusion matrix',\n    ylabel='True label',\n    xlabel='Predicted label'\n)\nparams = dict(rotation=45, ha='center', rotation_mode='anchor')\nplt.setp(ax.get_yticklabels(), **params)\nplt.setp(ax.get_xticklabels(), **params)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T10:46:05.916541Z","iopub.execute_input":"2021-10-19T10:46:05.923356Z","iopub.status.idle":"2021-10-19T10:46:07.694416Z","shell.execute_reply.started":"2021-10-19T10:46:05.923278Z","shell.execute_reply":"2021-10-19T10:46:07.693135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"10. Implement Grad-CAM, Grad-CAM++ and Score-CAM. First, implement the function to superimpose original image and heatmap of each cams.","metadata":{}},{"cell_type":"code","source":"def superimpose(img, cam):\n    \"\"\"superimpose original image and cam heatmap\"\"\"\n    \n    heatmap = cv2.resize(cam, (img.shape[1], img.shape[0]))\n    heatmap_ = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap_, cv2.COLORMAP_JET)\n\n    superimposed_img = heatmap * .5 + img * .5\n    superimposed_img = np.minimum(superimposed_img, 255.0).astype(np.uint8)  # scale 0 to 255  \n    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n    \n    return img, heatmap, superimposed_img","metadata":{"execution":{"iopub.status.busy":"2022-01-26T11:24:53.257193Z","iopub.execute_input":"2022-01-26T11:24:53.257492Z","iopub.status.idle":"2022-01-26T11:24:53.264444Z","shell.execute_reply.started":"2022-01-26T11:24:53.257432Z","shell.execute_reply":"2022-01-26T11:24:53.263456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extractROI(imgP, cam):\n#     img = cv2.imread(os.path.join(\"../input/processed-aptos/NewDataY/train_images\", imgP))\n    img = cv2.imread(imgP)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    heatmap = cv2.resize(cam, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_OCEAN)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_RGB2GRAY)\n    ret, mask = cv2.threshold(heatmap, 75, 255, cv2.THRESH_BINARY)\n    \n    extract = cv2.bitwise_and(img, img, mask = mask)\n    \n    return extract","metadata":{"execution":{"iopub.status.busy":"2022-01-26T11:24:53.599454Z","iopub.execute_input":"2022-01-26T11:24:53.599789Z","iopub.status.idle":"2022-01-26T11:24:53.607419Z","shell.execute_reply.started":"2022-01-26T11:24:53.599733Z","shell.execute_reply":"2022-01-26T11:24:53.606255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _plot(model, cam_func, img, imgP, cls_true):\n    \"\"\"plot original image, heatmap from cam and superimpose image\"\"\"\n    \n    # for cam\n    x = np.expand_dims(img, axis=0)\n    x = preprocess_input(copy.deepcopy(x))\n\n    # for superimpose\n    img = np.uint8(img)\n\n    # cam / superimpose\n    cls_pred, cam = cam_func(model=model, x=x, layer_name=model.layers[-2].name)\n    img, heatmap, superimposed_img = superimpose(img, cam)\n#     extract = extractROI(imgP, cam)\n\n    fig, axs = plt.subplots(ncols=3, figsize=(18, 8))\n\n    axs[0].imshow(img)\n    axs[0].set_title('original image')\n    axs[0].axis('off')\n\n    axs[1].imshow(heatmap)\n    axs[1].set_title('heatmap')\n    axs[1].axis('off')\n\n    axs[2].imshow(superimposed_img)\n    axs[2].set_title('superimposed image')\n    axs[2].axis('off')\n    \n#     axs[3].imshow(extract)\n#     axs[3].set_title('extractied image')\n#     axs[3].axis('off')\n\n    plt.suptitle('True label: ' + class_to_label[cls_true] + ' / Predicted label : ' + class_to_label[cls_pred])\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T11:35:53.89506Z","iopub.execute_input":"2022-01-26T11:35:53.895363Z","iopub.status.idle":"2022-01-26T11:35:53.90554Z","shell.execute_reply.started":"2022-01-26T11:35:53.89531Z","shell.execute_reply":"2022-01-26T11:35:53.90465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grad-CAM:\n","metadata":{}},{"cell_type":"code","source":"## Grad-CAM function\n\ndef grad_cam(model, x, layer_name):\n    \"\"\"Grad-CAM function\"\"\"\n    \n    cls = np.argmax(model.predict(x))\n    \n    y_c = model.output[0, cls]\n    conv_output = model.get_layer(layer_name).output\n    grads = K.gradients(y_c, conv_output)[0]\n\n    # Get outputs and grads\n    gradient_function = K.function([model.input], [conv_output, grads])\n    output, grads_val = gradient_function([x])\n    output, grads_val = output[0, :], grads_val[0, :, :, :]\n    \n    weights = np.mean(grads_val, axis=(0, 1)) # Passing through GlobalAveragePooling\n\n    cam = np.dot(output, weights) # multiply\n    cam = np.maximum(cam, 0)      # Passing through ReLU\n    cam /= np.max(cam)            # scale 0 to 1.0\n\n    return cls, cam","metadata":{"execution":{"iopub.status.busy":"2022-01-26T11:34:23.573445Z","iopub.execute_input":"2022-01-26T11:34:23.573785Z","iopub.status.idle":"2022-01-26T11:34:23.582709Z","shell.execute_reply.started":"2022-01-26T11:34:23.573732Z","shell.execute_reply":"2022-01-26T11:34:23.581694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 24\n# _plot(model=model, cam_func=grad_cam, img=Images[25], imgP=ImagePaths[25], cls_true=Classes[25])\n_plot(model=model, cam_func=grad_cam_plus_plus, img=Images[idx], imgP=ImagePaths[idx], cls_true=Classes[idx])","metadata":{"execution":{"iopub.status.busy":"2022-01-26T11:40:56.933999Z","iopub.execute_input":"2022-01-26T11:40:56.9343Z","iopub.status.idle":"2022-01-26T11:41:00.319455Z","shell.execute_reply.started":"2022-01-26T11:40:56.934248Z","shell.execute_reply":"2022-01-26T11:41:00.318705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Grad-CAM function\n\ndef grad_cam(model, x, layer_name):\n    \"\"\"Grad-CAM function\"\"\"\n    \n    cls = np.argmax(model.predict(x))\n    \n    y_c = model.output[0, cls]\n    conv_output = model.get_layer(layer_name).output\n    grads = K.gradients(y_c, conv_output)[0]\n\n    # Get outputs and grads\n    gradient_function = K.function([model.input], [conv_output, grads])\n    output, grads_val = gradient_function([x])\n    output, grads_val = output[0, :], grads_val[0, :, :, :]\n    \n    weights = np.mean(grads_val, axis=(0, 1)) # Passing through GlobalAveragePooling\n\n    cam = np.dot(output, weights) # multiply\n    cam = np.maximum(cam, 0)      # Passing through ReLU\n    cam /= np.max(cam)            # scale 0 to 1.0\n\n    return cls, cam\n\n## Grad-CAM++ function\n\ndef grad_cam_plus_plus(model, x, layer_name):\n    \"\"\"Grad-CAM++ function\"\"\"\n    \n    cls = np.argmax(model.predict(x))\n    y_c = model.output[0, cls]\n    conv_output = model.get_layer(layer_name).output\n    grads = K.gradients(y_c, conv_output)[0]\n\n    first = K.exp(y_c) * grads\n    second = K.exp(y_c) * grads * grads\n    third = K.exp(y_c) * grads * grads * grads\n\n    gradient_function = K.function([model.input], [y_c, first, second, third, conv_output, grads])\n    y_c, conv_first_grad, conv_second_grad, conv_third_grad, conv_output, grads_val = gradient_function([x])\n    global_sum = np.sum(conv_output[0].reshape((-1,conv_first_grad[0].shape[2])), axis=0)\n\n    alpha_num = conv_second_grad[0]\n    alpha_denom = conv_second_grad[0] * 2.0 + conv_third_grad[0] * global_sum.reshape((1, 1, conv_first_grad[0].shape[2]))\n    alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, np.ones(alpha_denom.shape))\n    alphas = alpha_num / alpha_denom # 0\n\n\n    weights = np.maximum(conv_first_grad[0], 0.0)\n    alpha_normalization_constant = np.sum(np.sum(alphas, axis=0), axis=0) # 0\n    alphas /= alpha_normalization_constant.reshape((1, 1, conv_first_grad[0].shape[2])) # NAN\n    deep_linearization_weights = np.sum((weights * alphas).reshape((-1, conv_first_grad[0].shape[2])), axis=0)\n\n    cam = np.sum(deep_linearization_weights * conv_output[0], axis=2)\n    cam = np.maximum(cam, 0) # Passing through ReLU\n    cam /= np.max(cam)       # scale 0 to 1.0  \n\n    return cls, cam\n\n## Score-CAM function\n\ndef softmax(x):\n    \"\"\"softmax\"\"\"\n    \n    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n\ndef score_cam(model, x, layer_name, max_N=-1):\n    \"\"\"Score-CAM function\"\"\"\n\n    cls = np.argmax(model.predict(x))\n    act_map_array = Model(inputs=model.input, outputs=model.get_layer(layer_name).output).predict(x)\n    \n    # extract effective maps\n    if max_N != -1:\n        act_map_std_list = [np.std(act_map_array[0, :, :, k]) for k in range(act_map_array.shape[3])]\n        unsorted_max_indices = np.argpartition(-np.array(act_map_std_list), max_N)[:max_N]\n        max_N_indices = unsorted_max_indices[np.argsort(-np.array(act_map_std_list)[unsorted_max_indices])]\n        act_map_array = act_map_array[:, :, :, max_N_indices]\n\n    input_shape = model.layers[0].output_shape[1:]  # get input shape\n    \n    # 1. upsampled to original input size\n    act_map_resized_list = [cv2.resize(act_map_array[0,:,:,k], input_shape[:2], interpolation=cv2.INTER_LINEAR) for k in range(act_map_array.shape[3])]\n    \n    # 2. normalize the raw activation value in each activation map into [0, 1]\n    act_map_normalized_list = []\n    for act_map_resized in act_map_resized_list:\n        if np.max(act_map_resized) - np.min(act_map_resized) != 0:\n            act_map_normalized = act_map_resized / (np.max(act_map_resized) - np.min(act_map_resized))\n        else:\n            act_map_normalized = act_map_resized\n        act_map_normalized_list.append(act_map_normalized)\n        \n    # 3. project highlighted area in the activation map to original input space by multiplying the normalized activation map\n    masked_input_list = []\n    for act_map_normalized in act_map_normalized_list:\n        masked_input = np.copy(x)\n        for k in range(3):\n            masked_input[0, :, :, k] *= act_map_normalized\n        masked_input_list.append(masked_input)\n    masked_input_array = np.concatenate(masked_input_list, axis=0)\n    \n    # 4. feed masked inputs into CNN model and softmax\n    pred_from_masked_input_array = softmax(model.predict(masked_input_array))\n    \n    # 5. define weight as the score of target class\n    weights = pred_from_masked_input_array[:, cls]\n    \n    # 6. get final class discriminative localization map as linear weighted combination of all activation maps\n    cam = np.dot(act_map_array[0, :, :, :], weights)\n    cam = np.maximum(0, cam) # Passing through ReLU\n    cam /= np.max(cam) # scale 0 to 1.0\n    \n    return cls, cam","metadata":{"execution":{"iopub.status.busy":"2022-01-26T11:27:06.087538Z","iopub.execute_input":"2022-01-26T11:27:06.087855Z","iopub.status.idle":"2022-01-26T11:27:06.121262Z","shell.execute_reply.started":"2022-01-26T11:27:06.087801Z","shell.execute_reply":"2022-01-26T11:27:06.120043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kirschFilter(image):\n        gray = image\n        if gray.ndim > 2:\n            raise Exception(\"illegal argument: input must be a single channel image (gray)\")\n        kernelG1 = np.array([[5, 5, 5],\n                             [-3, 0, -3],\n                             [-3, -3, -3]], dtype=np.float32)\n        kernelG2 = np.array([[5, 5, -3],\n                             [5, 0, -3],\n                             [-3, -3, -3]], dtype=np.float32)\n        kernelG3 = np.array([[5, -3, -3],\n                             [5, 0, -3],\n                             [5, -3, -3]], dtype=np.float32)\n        kernelG4 = np.array([[-3, -3, -3],\n                             [5, 0, -3],\n                             [5, 5, -3]], dtype=np.float32)\n        kernelG5 = np.array([[-3, -3, -3],\n                             [-3, 0, -3],\n                             [5, 5, 5]], dtype=np.float32)\n        kernelG6 = np.array([[-3, -3, -3],\n                             [-3, 0, 5],\n                             [-3, 5, 5]], dtype=np.float32)\n        kernelG7 = np.array([[-3, -3, 5],\n                             [-3, 0, 5],\n                             [-3, -3, 5]], dtype=np.float32)\n        kernelG8 = np.array([[-3, 5, 5],\n                             [-3, 0, 5],\n                             [-3, -3, -3]], dtype=np.float32)\n\n        g1 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG1), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g2 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG2), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g3 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG3), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g4 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG4), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g5 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG5), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g6 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG6), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g7 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG7), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g8 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG8), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        magn = cv2.max(g1, cv2.max(g2, cv2.max(g3, cv2.max(g4, cv2.max(g5, cv2.max(g6, cv2.max(g7, g8)))))))\n        return magn\n\ndef getBloodVessels(cvImage):\n    nImage = np.array(cvImage)\n    gImage = nImage[:, :, 1].astype('uint8')\n    \n    hImage = cv2.equalizeHist(gImage)\n    mImage = kirschFilter(hImage)\n    \n    ret, tImage = cv2.threshold(mImage, 160, 180, cv2.THRESH_BINARY_INV)\n    cImage = morphology.remove_small_objects(tImage, min_size=150, connectivity=100)\n    return cImage\n\ndef getExudates(cvImage):\n    nImage = np.array(cvImage)\n    gImage = nImage[:, :, 1].astype('uint8')\n    \n    clahe = cv2.createCLAHE()\n    cImage = clahe.apply(gImage)\n    \n    dImage = cv2.dilate(cImage, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6, 6)))\n    ret, tImage = cv2.threshold(dImage, 220, 220, cv2.THRESH_BINARY)\n    mImage = cv2.medianBlur(tImage, 5)\n    \n#     _, mImage = cv2.threshold(mImage, 0, 255, cv2.THRESH_BINARY_INV)\n    return mImage\n\ndef adjust_gamma(image, gamma=1.0):\n   table = np.array([((i / 255.0) ** gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n   return cv2.LUT(image, table)\n\ndef getMicroaneurysms(image):\n    r, g, b=cv2.split(image)\n    comp = 255 - g\n    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8,8))\n    histe = clahe.apply(comp)\n    adjustImage = adjust_gamma(histe, gamma=3)\n    \n    comp = 255 - adjustImage\n    J = adjust_gamma(comp, gamma=4)\n    J = 255 - J\n    J = adjust_gamma(J, gamma=4)\n    \n    K = np.ones((11,11), np.float32)\n    L = cv2.filter2D(J,-1,K)\n    \n    ret3, thresh2 = cv2.threshold(L,125,255,cv2.THRESH_BINARY|cv2.THRESH_OTSU)\n    kernel2 = np.ones((9,9),np.uint8)\n    tophat = cv2.morphologyEx(thresh2, cv2.MORPH_TOPHAT, kernel2)\n    kernel3 = np.ones((7,7),np.uint8)\n    opening = cv2.morphologyEx(tophat, cv2.MORPH_OPEN, kernel3)\n    \n    return opening\n\ndef getBV(image):\n    b,green_fundus,r = cv2.split(image)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    contrast_enhanced_green_fundus = clahe.apply(green_fundus)\n\n    r1 = cv2.morphologyEx(contrast_enhanced_green_fundus, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5)), iterations = 1)\n    R1 = cv2.morphologyEx(r1, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5)), iterations = 1)\n    r2 = cv2.morphologyEx(R1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11)), iterations = 1)\n    R2 = cv2.morphologyEx(r2, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11)), iterations = 1)\n    r3 = cv2.morphologyEx(R2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(23,23)), iterations = 1)\n    R3 = cv2.morphologyEx(r3, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(23,23)), iterations = 1)\t\n    f4 = cv2.subtract(R3,contrast_enhanced_green_fundus)\n    f5 = clahe.apply(f4)\n    bloodvessel=\"aa\"\n    # removing very small contours through area parameter noise removal\n    ret,f6 = cv2.threshold(f5,15,255,cv2.THRESH_BINARY)\n    mask = np.ones(f5.shape[:2], dtype=\"uint8\") * 255\n    contours, hierarchy = cv2.findContours(f6.copy(),cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n    for cnt in contours:\n        if cv2.contourArea(cnt) <= 200:\n            cv2.drawContours(mask, [cnt], -1, 0, -1)\n    im = cv2.bitwise_and(f5, f5, mask=mask)\n    ret,fin = cv2.threshold(im,15,255,cv2.THRESH_BINARY_INV)\n    newfin = cv2.erode(fin, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)), iterations=1)\n\n    # removing blobs of unwanted bigger chunks taking in consideration they are not straight lines like blood vessels and also in an interval of area\n    fundus_eroded = cv2.bitwise_not(newfin)\t\n    xmask = np.ones(image.shape[:2], dtype=\"uint8\") * 255\n    xcontours, xhierarchy = cv2.findContours(fundus_eroded.copy(),cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n    for cnt in xcontours:\n        shape = \"unidentified\"\n        peri = cv2.arcLength(cnt, True)\n        approx = cv2.approxPolyDP(cnt, 0.04 * peri, False)   \n        if len(approx) > 4 and cv2.contourArea(cnt) <= 3000 and cv2.contourArea(cnt) >= 100:\n            shape = \"circle\"\n        else:\n            shape = \"veins\"\n        if(shape==\"circle\"):\n            cv2.drawContours(xmask, [cnt], -1, 0, -1)\n\n    finimage = cv2.bitwise_and(fundus_eroded,fundus_eroded,mask=xmask)\n    blood_vessels = cv2.bitwise_not(finimage)\n    return blood_vessels","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:34:49.029083Z","iopub.execute_input":"2021-11-08T16:34:49.029448Z","iopub.status.idle":"2021-11-08T16:34:49.085243Z","shell.execute_reply.started":"2021-11-08T16:34:49.029395Z","shell.execute_reply":"2021-11-08T16:34:49.084491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def superimpose(img, cam):\n    \"\"\"superimpose original image and cam heatmap\"\"\"\n    \n    heatmap = cv2.resize(cam, (img.shape[1], img.shape[0]))\n    heatmap_ = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap_, cv2.COLORMAP_JET)\n\n    superimposed_img = heatmap * .5 + img * .5\n    superimposed_img = np.minimum(superimposed_img, 255.0).astype(np.uint8)  # scale 0 to 255  \n    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n    \n    return img, heatmap, superimposed_img\n\ndef extractROI(imgP, cam):\n#     img = cv2.imread(os.path.join(\"../input/processed-aptos/NewDataY/train_images\", imgP))\n    img = cv2.imread(imgP)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    heatmap = cv2.resize(cam, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_OCEAN)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_RGB2GRAY)\n    ret, mask = cv2.threshold(heatmap, 75, 255, cv2.THRESH_BINARY)\n    \n    extract = cv2.bitwise_and(img, img, mask = mask)\n    \n    return extract","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:34:49.568156Z","iopub.execute_input":"2021-11-08T16:34:49.568737Z","iopub.status.idle":"2021-11-08T16:34:49.581893Z","shell.execute_reply.started":"2021-11-08T16:34:49.568675Z","shell.execute_reply":"2021-11-08T16:34:49.581145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def countWP(image):\n    if type(image) == str:\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n    size = image.shape[0] * image.shape[1]\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    image = cv2.GaussianBlur(image, (3,3), 0)\n    _, thresh = cv2.threshold(image, 16, 255, cv2.THRESH_BINARY)\n        \n    cnts, hier = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    white_size = 0\n    count = 0\n    for cnt in cnts:\n        white_size += cv2.contourArea(cnt)\n        count += 1\n    \n    return str(count)\n\ndef getWP(image):\n    if type(image) == str:\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n#     image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    wp = np.sum(image != 0)\n    bp = np.sum(image == 0)\n    \n    wp = round((wp/(wp+bp))*100, 3)\n    return str(wp)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:34:50.032808Z","iopub.execute_input":"2021-11-08T16:34:50.033522Z","iopub.status.idle":"2021-11-08T16:34:50.051611Z","shell.execute_reply.started":"2021-11-08T16:34:50.033463Z","shell.execute_reply":"2021-11-08T16:34:50.050617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.load_model('../input/idrid-segmentation/model864.h5')\n\ndef _check(model, layer_name, img, imgP, maP, heP, exP, sN):\n    \"\"\"compare Grad-CAM / Grad-CAM++ / Score-CAM on target class images\"\"\"\n    \n    fig, axs = plt.subplots(ncols=4, nrows=3, figsize=(36,16)) #(25, 9)\n\n    # for cam\n    x = np.expand_dims(img, axis=0)\n    x = preprocess_input(copy.deepcopy(x))\n    label = class_to_label[np.argmax(model.predict(x))]\n\n    # Original\n    axs[0, 0].imshow(np.uint8(img))\n    axs[0, 0].set_title('Input Retina')\n\n    # Grad-CAM\n    cls_pred, camG = grad_cam(model=model, x=x, layer_name=layer_name)\n    _, _, img_grad_cam = superimpose(img, camG)\n    \n    gray = cv2.cvtColor(extractROI(imgP, camG), cv2.COLOR_BGR2GRAY)\n    _, roi = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n    wpG = getWP(roi)\n    \n    axs[0, 1].imshow(img_grad_cam)\n    axs[0, 1].set_title('Grad-CAM Map')\n\n    # Grad-CAM++\n    cls_pred, camGPP = grad_cam_plus_plus(model=model, x=x, layer_name=layer_name)\n    _, _, img_grad_cam_plus_plus = superimpose(img, camGPP)\n    \n    gray = cv2.cvtColor(extractROI(imgP, camGPP), cv2.COLOR_BGR2GRAY)\n    _, roi = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n    wpGPP = getWP(roi)\n    \n    axs[0, 2].imshow(img_grad_cam_plus_plus)\n    axs[0, 2].set_title('Grad-CAM++ Map')\n\n    # Score-CAM\n    cls_pred, camS = score_cam(model=model, x=x, layer_name=layer_name)\n    _, _, img_score_cam = superimpose(img, camS)\n    \n    gray = cv2.cvtColor(extractROI(imgP, camS), cv2.COLOR_BGR2GRAY)\n    _, roi = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n    wpS = getWP(roi)\n    \n    axs[0, 3].imshow(img_score_cam)\n    axs[0, 3].set_title('Score-CAM Map')\n    \n    wp = str(max(float(wpG), float(wpGPP), float(wpS)))\n    if wp == wpG:\n        cam = camG\n    elif wp == wpGPP:\n        cam = camGPP\n    elif wp == wpS:\n        cam = camS\n\n    # Extraction\n    img_ROI = extractROI(imgP, cam)\n    axs[1, 0].imshow(img_ROI)\n    axs[1, 0].set_title('Extracted Retina')\n\n    # Hemorrhages\n    imageHE = extractROI(heP, cam)\n    wsHE = getWP(imageHE)\n    axs[1, 1].imshow(np.uint8(imageHE))\n    axs[1, 1].set_title('Extracted Hemorrhages - {}%'.format(wsHE))\n\n    # Exudates\n    imageEX = extractROI(exP, cam)\n    wsEX = getWP(imageEX)\n    axs[1, 2].imshow(np.uint8(imageEX))\n    axs[1, 2].set_title('Extracted Exudates - {}%'.format(wsEX))\n\n    # Microaneurysms\n    imageMA = extractROI(maP, cam)\n    wsMA = getWP(imageMA)\n    cMA = countWP(imageMA)\n    axs[1, 3].imshow(np.uint8(imageMA))\n    axs[1, 3].set_title('Extracted Microaneurysms - {}% - {}'.format(wsMA, cMA))\n    \n    # OG Retina\n    imgO = cv2.imread(imgP)\n    imgO = cv2.cvtColor(imgO, cv2.COLOR_BGR2RGB)\n    axs[2, 0].imshow(imgO)\n    axs[2, 0].set_title('Original Retina')\n\n    # Hemorrhages\n    imageHE = cv2.imread(heP)\n    imageHE = cv2.cvtColor(imageHE, cv2.COLOR_BGR2RGB)\n    wsHE = getWP(imageHE)\n    axs[2, 1].imshow(np.uint8(imageHE))\n    axs[2, 1].set_title('Original Hemorrhages - {}%'.format(wsHE))\n\n    # Exudates\n    imageEX = cv2.imread(exP)\n    imageEX = cv2.cvtColor(imageEX, cv2.COLOR_BGR2RGB)\n    wsEX = getWP(imageEX)\n    axs[2, 2].imshow(np.uint8(imageEX))\n    axs[2, 2].set_title('Original Exudates - {}%'.format(wsEX))\n\n    # Microaneurysms\n    imageMA = cv2.imread(maP)\n    imageMA = cv2.cvtColor(imageMA, cv2.COLOR_BGR2RGB)\n    wsMA = getWP(imageMA)\n    cMA = countWP(imageMA)\n    axs[2, 3].imshow(np.uint8(imageMA))\n    axs[2, 3].set_title('Original Microaneurysms - {}% - {}'.format(wsMA, cMA))\n    \n    ws = ' + '.join([wsHE, wsEX, wsMA, cMA])\n    fig.savefig('Images/'+sN+' _ '+label+' - '+ws+'.jpg')","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:34:50.876727Z","iopub.execute_input":"2021-11-08T16:34:50.880528Z","iopub.status.idle":"2021-11-08T16:35:13.253546Z","shell.execute_reply.started":"2021-11-08T16:34:50.880458Z","shell.execute_reply":"2021-11-08T16:35:13.252753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name_ = '25'\nimgP = \"../input/idrid-segmentation/SegmentData/Original Images/Training Set/IDRiD_\"+name_+\".jpg\" \n\nimageT = cv2.imread(\"../input/idrid-segmentation/SegmentData/Original Images/Training Set/IDRiD_\"+name_+\".jpg\")\nimageT = cv2.cvtColor(imageT, cv2.COLOR_BGR2RGB)\nimageT = cv2.resize(imageT, (112, 112))\n\nmaP = \"../input/idrid-segmentation/SegmentData/Segmentation Groundtruths/Training Set/MA/IDRiD_\"+name_+\"_MA.tif\" \nheP = \"../input/idrid-segmentation/SegmentData/Segmentation Groundtruths/Training Set/HE/IDRiD_\"+name_+\"_HE.tif\"\nexP = \"../input/idrid-segmentation/SegmentData/Segmentation Groundtruths/Training Set/EX/IDRiD_\"+name_+\"_EX.tif\"\n\n_check(model, model.layers[-2].name, imageT, imgP, maP, heP, exP, name_)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:01:17.9724Z","iopub.execute_input":"2021-11-08T16:01:17.972695Z","iopub.status.idle":"2021-11-08T16:01:42.31501Z","shell.execute_reply.started":"2021-11-08T16:01:17.972648Z","shell.execute_reply":"2021-11-08T16:01:42.314183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir Images\n\nfor idx in tqdm(range(55, 55)):\n    name_ = str(idx).zfill(2)\n    imgP = \"../input/idrid-segmentation/SegmentData/Original Images/Training Set/IDRiD_\"+name_+\".jpg\" \n\n    imageT = cv2.imread(\"../input/idrid-segmentation/SegmentData/Original Images/Training Set/IDRiD_\"+name_+\".jpg\")\n    imageT = cv2.cvtColor(imageT, cv2.COLOR_BGR2RGB)\n    imageT = cv2.resize(imageT, (112, 112))\n\n    maP = \"../input/idrid-segmentation/SegmentData/Segmentation Groundtruths/Training Set/MA/IDRiD_\"+name_+\"_MA.tif\" \n    heP = \"../input/idrid-segmentation/SegmentData/Segmentation Groundtruths/Training Set/HE/IDRiD_\"+name_+\"_HE.tif\"\n    exP = \"../input/idrid-segmentation/SegmentData/Segmentation Groundtruths/Training Set/EX/IDRiD_\"+name_+\"_EX.tif\"\n\n    _check(model, model.layers[-2].name, imageT, imgP, maP, heP, exP, name_)\n    \nfor idx in tqdm(range(55, 82)):\n    name_ = str(idx).zfill(2)\n    imgP = \"../input/idrid-segmentation/SegmentData/Original Images/Testing Set/IDRiD_\"+name_+\".jpg\" \n\n    imageT = cv2.imread(\"../input/idrid-segmentation/SegmentData/Original Images/Testing Set/IDRiD_\"+name_+\".jpg\")\n    imageT = cv2.cvtColor(imageT, cv2.COLOR_BGR2RGB)\n    imageT = cv2.resize(imageT, (112, 112))\n\n    maP = \"../input/idrid-segmentation/SegmentData/Segmentation Groundtruths/Testing Set/MA/IDRiD_\"+name_+\"_MA.tif\" \n    heP = \"../input/idrid-segmentation/SegmentData/Segmentation Groundtruths/Testing Set/HE/IDRiD_\"+name_+\"_HE.tif\"\n    exP = \"../input/idrid-segmentation/SegmentData/Segmentation Groundtruths/Testing Set/EX/IDRiD_\"+name_+\"_EX.tif\"\n\n    _check(model, model.layers[-2].name, imageT, imgP, maP, heP, exP, name_)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:35:13.255181Z","iopub.execute_input":"2021-11-08T16:35:13.255489Z","iopub.status.idle":"2021-11-08T16:45:40.5906Z","shell.execute_reply.started":"2021-11-08T16:35:13.255441Z","shell.execute_reply":"2021-11-08T16:45:40.589775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install zip\n!zip -r imageTrain.zip Images/","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:45:40.592374Z","iopub.execute_input":"2021-11-08T16:45:40.592933Z","iopub.status.idle":"2021-11-08T16:45:44.720143Z","shell.execute_reply.started":"2021-11-08T16:45:40.592881Z","shell.execute_reply":"2021-11-08T16:45:44.719408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:30:30.583607Z","iopub.execute_input":"2021-10-07T10:30:30.583946Z","iopub.status.idle":"2021-10-07T10:30:32.458502Z","shell.execute_reply.started":"2021-10-07T10:30:30.583894Z","shell.execute_reply":"2021-10-07T10:30:32.45746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:28:47.324597Z","iopub.status.idle":"2021-10-07T10:28:47.325344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Grad-CAM++ function\n\ndef grad_cam_plus_plus(model, x, layer_name):\n    \"\"\"Grad-CAM++ function\"\"\"\n    \n    cls = np.argmax(model.predict(x))\n    y_c = model.output[0, cls]\n    conv_output = model.get_layer(layer_name).output\n    grads = K.gradients(y_c, conv_output)[0]\n\n    first = K.exp(y_c) * grads\n    second = K.exp(y_c) * grads * grads\n    third = K.exp(y_c) * grads * grads * grads\n\n    gradient_function = K.function([model.input], [y_c, first, second, third, conv_output, grads])\n    y_c, conv_first_grad, conv_second_grad, conv_third_grad, conv_output, grads_val = gradient_function([x])\n    global_sum = np.sum(conv_output[0].reshape((-1,conv_first_grad[0].shape[2])), axis=0)\n\n    alpha_num = conv_second_grad[0]\n    alpha_denom = conv_second_grad[0] * 2.0 + conv_third_grad[0] * global_sum.reshape((1, 1, conv_first_grad[0].shape[2]))\n    alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, np.ones(alpha_denom.shape))\n    alphas = alpha_num / alpha_denom # 0\n\n\n    weights = np.maximum(conv_first_grad[0], 0.0)\n    alpha_normalization_constant = np.sum(np.sum(alphas, axis=0), axis=0) # 0\n    alphas /= alpha_normalization_constant.reshape((1, 1, conv_first_grad[0].shape[2])) # NAN\n    deep_linearization_weights = np.sum((weights * alphas).reshape((-1, conv_first_grad[0].shape[2])), axis=0)\n\n    cam = np.sum(deep_linearization_weights * conv_output[0], axis=2)\n    cam = np.maximum(cam, 0) # Passing through ReLU\n    cam /= np.max(cam)       # scale 0 to 1.0  \n\n    return cls, cam","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:17:07.271298Z","iopub.execute_input":"2021-11-03T11:17:07.271546Z","iopub.status.idle":"2021-11-03T11:17:07.287037Z","shell.execute_reply.started":"2021-11-03T11:17:07.271496Z","shell.execute_reply":"2021-11-03T11:17:07.286159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_plot(model=model, cam_func=grad_cam_plus_plus, img=Images[25], imgP=ImagePaths[25], cls_true=Classes[25])","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:09:31.631459Z","iopub.execute_input":"2021-10-07T10:09:31.631748Z","iopub.status.idle":"2021-10-07T10:09:34.474237Z","shell.execute_reply.started":"2021-10-07T10:09:31.631695Z","shell.execute_reply":"2021-10-07T10:09:34.47295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Score-CAM:\n","metadata":{}},{"cell_type":"code","source":"## Score-CAM function\n\ndef softmax(x):\n    \"\"\"softmax\"\"\"\n    \n    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n\ndef score_cam(model, x, layer_name, max_N=-1):\n    \"\"\"Score-CAM function\"\"\"\n\n    cls = np.argmax(model.predict(x))\n    act_map_array = Model(inputs=model.input, outputs=model.get_layer(layer_name).output).predict(x)\n    \n    # extract effective maps\n    if max_N != -1:\n        act_map_std_list = [np.std(act_map_array[0, :, :, k]) for k in range(act_map_array.shape[3])]\n        unsorted_max_indices = np.argpartition(-np.array(act_map_std_list), max_N)[:max_N]\n        max_N_indices = unsorted_max_indices[np.argsort(-np.array(act_map_std_list)[unsorted_max_indices])]\n        act_map_array = act_map_array[:, :, :, max_N_indices]\n\n    input_shape = model.layers[0].output_shape[1:]  # get input shape\n    \n    # 1. upsampled to original input size\n    act_map_resized_list = [cv2.resize(act_map_array[0,:,:,k], input_shape[:2], interpolation=cv2.INTER_LINEAR) for k in range(act_map_array.shape[3])]\n    \n    # 2. normalize the raw activation value in each activation map into [0, 1]\n    act_map_normalized_list = []\n    for act_map_resized in act_map_resized_list:\n        if np.max(act_map_resized) - np.min(act_map_resized) != 0:\n            act_map_normalized = act_map_resized / (np.max(act_map_resized) - np.min(act_map_resized))\n        else:\n            act_map_normalized = act_map_resized\n        act_map_normalized_list.append(act_map_normalized)\n        \n    # 3. project highlighted area in the activation map to original input space by multiplying the normalized activation map\n    masked_input_list = []\n    for act_map_normalized in act_map_normalized_list:\n        masked_input = np.copy(x)\n        for k in range(3):\n            masked_input[0, :, :, k] *= act_map_normalized\n        masked_input_list.append(masked_input)\n    masked_input_array = np.concatenate(masked_input_list, axis=0)\n    \n    # 4. feed masked inputs into CNN model and softmax\n    pred_from_masked_input_array = softmax(model.predict(masked_input_array))\n    \n    # 5. define weight as the score of target class\n    weights = pred_from_masked_input_array[:, cls]\n    \n    # 6. get final class discriminative localization map as linear weighted combination of all activation maps\n    cam = np.dot(act_map_array[0, :, :, :], weights)\n    cam = np.maximum(0, cam) # Passing through ReLU\n    cam /= np.max(cam) # scale 0 to 1.0\n    \n    return cls, cam","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:17:07.28845Z","iopub.execute_input":"2021-11-03T11:17:07.288992Z","iopub.status.idle":"2021-11-03T11:17:07.307973Z","shell.execute_reply.started":"2021-11-03T11:17:07.288943Z","shell.execute_reply":"2021-11-03T11:17:07.307397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_plot(model=model, cam_func=score_cam, img=Images[25], imgP=ImagePaths[25], cls_true=Classes[25])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:17:07.310439Z","iopub.execute_input":"2021-11-03T11:17:07.310669Z","iopub.status.idle":"2021-11-03T11:17:07.772555Z","shell.execute_reply.started":"2021-11-03T11:17:07.310625Z","shell.execute_reply":"2021-11-03T11:17:07.77149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kirschFilter(image):\n        gray = image\n        if gray.ndim > 2:\n            raise Exception(\"illegal argument: input must be a single channel image (gray)\")\n        kernelG1 = np.array([[5, 5, 5],\n                             [-3, 0, -3],\n                             [-3, -3, -3]], dtype=np.float32)\n        kernelG2 = np.array([[5, 5, -3],\n                             [5, 0, -3],\n                             [-3, -3, -3]], dtype=np.float32)\n        kernelG3 = np.array([[5, -3, -3],\n                             [5, 0, -3],\n                             [5, -3, -3]], dtype=np.float32)\n        kernelG4 = np.array([[-3, -3, -3],\n                             [5, 0, -3],\n                             [5, 5, -3]], dtype=np.float32)\n        kernelG5 = np.array([[-3, -3, -3],\n                             [-3, 0, -3],\n                             [5, 5, 5]], dtype=np.float32)\n        kernelG6 = np.array([[-3, -3, -3],\n                             [-3, 0, 5],\n                             [-3, 5, 5]], dtype=np.float32)\n        kernelG7 = np.array([[-3, -3, 5],\n                             [-3, 0, 5],\n                             [-3, -3, 5]], dtype=np.float32)\n        kernelG8 = np.array([[-3, 5, 5],\n                             [-3, 0, 5],\n                             [-3, -3, -3]], dtype=np.float32)\n\n        g1 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG1), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g2 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG2), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g3 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG3), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g4 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG4), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g5 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG5), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g6 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG6), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g7 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG7), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        g8 = cv2.normalize(cv2.filter2D(gray, cv2.CV_32F, kernelG8), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n        magn = cv2.max(g1, cv2.max(g2, cv2.max(g3, cv2.max(g4, cv2.max(g5, cv2.max(g6, cv2.max(g7, g8)))))))\n        return magn\n\ndef getBloodVessels(cvImage):\n    nImage = np.array(cvImage)\n    gImage = nImage[:, :, 1].astype('uint8')\n    \n    hImage = cv2.equalizeHist(gImage)\n    mImage = kirschFilter(hImage)\n    \n    ret, tImage = cv2.threshold(mImage, 160, 180, cv2.THRESH_BINARY_INV)\n    cImage = morphology.remove_small_objects(tImage, min_size=150, connectivity=100)\n    return cImage\n\ndef getExudates(cvImage):\n    nImage = np.array(cvImage)\n    gImage = nImage[:, :, 1].astype('uint8')\n    \n    clahe = cv2.createCLAHE()\n    cImage = clahe.apply(gImage)\n    \n    dImage = cv2.dilate(cImage, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6, 6)))\n    ret, tImage = cv2.threshold(dImage, 220, 220, cv2.THRESH_BINARY)\n    mImage = cv2.medianBlur(tImage, 5)\n    \n#     _, mImage = cv2.threshold(mImage, 0, 255, cv2.THRESH_BINARY_INV)\n    return mImage\n\ndef adjust_gamma(image, gamma=1.0):\n   table = np.array([((i / 255.0) ** gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n   return cv2.LUT(image, table)\n\ndef getMicroaneurysms(image):\n    r, g, b=cv2.split(image)\n    comp = 255 - g\n    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8,8))\n    histe = clahe.apply(comp)\n    adjustImage = adjust_gamma(histe, gamma=3)\n    \n    comp = 255 - adjustImage\n    J = adjust_gamma(comp, gamma=4)\n    J = 255 - J\n    J = adjust_gamma(J, gamma=4)\n    \n    K = np.ones((11,11), np.float32)\n    L = cv2.filter2D(J,-1,K)\n    \n    ret3, thresh2 = cv2.threshold(L,125,255,cv2.THRESH_BINARY|cv2.THRESH_OTSU)\n    kernel2 = np.ones((9,9),np.uint8)\n    tophat = cv2.morphologyEx(thresh2, cv2.MORPH_TOPHAT, kernel2)\n    kernel3 = np.ones((7,7),np.uint8)\n    opening = cv2.morphologyEx(tophat, cv2.MORPH_OPEN, kernel3)\n    \n    return opening\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:17:43.274311Z","iopub.execute_input":"2021-11-03T11:17:43.274644Z","iopub.status.idle":"2021-11-03T11:17:43.310987Z","shell.execute_reply.started":"2021-11-03T11:17:43.274589Z","shell.execute_reply":"2021-11-03T11:17:43.309248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getBV(image):\n    b,green_fundus,r = cv2.split(image)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    contrast_enhanced_green_fundus = clahe.apply(green_fundus)\n\n    r1 = cv2.morphologyEx(contrast_enhanced_green_fundus, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5)), iterations = 1)\n    R1 = cv2.morphologyEx(r1, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5)), iterations = 1)\n    r2 = cv2.morphologyEx(R1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11)), iterations = 1)\n    R2 = cv2.morphologyEx(r2, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11)), iterations = 1)\n    r3 = cv2.morphologyEx(R2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(23,23)), iterations = 1)\n    R3 = cv2.morphologyEx(r3, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(23,23)), iterations = 1)\t\n    f4 = cv2.subtract(R3,contrast_enhanced_green_fundus)\n    f5 = clahe.apply(f4)\n    bloodvessel=\"aa\"\n    # removing very small contours through area parameter noise removal\n    ret,f6 = cv2.threshold(f5,15,255,cv2.THRESH_BINARY)\n    mask = np.ones(f5.shape[:2], dtype=\"uint8\") * 255\n    contours, hierarchy = cv2.findContours(f6.copy(),cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n    for cnt in contours:\n        if cv2.contourArea(cnt) <= 200:\n            cv2.drawContours(mask, [cnt], -1, 0, -1)\n    im = cv2.bitwise_and(f5, f5, mask=mask)\n    ret,fin = cv2.threshold(im,15,255,cv2.THRESH_BINARY_INV)\n    newfin = cv2.erode(fin, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)), iterations=1)\n\n    # removing blobs of unwanted bigger chunks taking in consideration they are not straight lines like blood vessels and also in an interval of area\n    fundus_eroded = cv2.bitwise_not(newfin)\t\n    xmask = np.ones(image.shape[:2], dtype=\"uint8\") * 255\n    xcontours, xhierarchy = cv2.findContours(fundus_eroded.copy(),cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n    for cnt in xcontours:\n        shape = \"unidentified\"\n        peri = cv2.arcLength(cnt, True)\n        approx = cv2.approxPolyDP(cnt, 0.04 * peri, False)   \n        if len(approx) > 4 and cv2.contourArea(cnt) <= 3000 and cv2.contourArea(cnt) >= 100:\n            shape = \"circle\"\n        else:\n            shape = \"veins\"\n        if(shape==\"circle\"):\n            cv2.drawContours(xmask, [cnt], -1, 0, -1)\n\n    finimage = cv2.bitwise_and(fundus_eroded,fundus_eroded,mask=xmask)\n    blood_vessels = cv2.bitwise_not(finimage)\n    return blood_vessels","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:17:43.881543Z","iopub.execute_input":"2021-11-03T11:17:43.881836Z","iopub.status.idle":"2021-11-03T11:17:43.901896Z","shell.execute_reply.started":"2021-11-03T11:17:43.881787Z","shell.execute_reply":"2021-11-03T11:17:43.901171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"11. Compare each visual methods. ","metadata":{}},{"cell_type":"code","source":"## compare Grad-CAM / Grad-CAM++ / Score-CAM\n\ndef _compare(model, layer_name, target_cls):\n    \"\"\"compare Grad-CAM / Grad-CAM++ / Score-CAM on target class images\"\"\"\n    \n    indices = np.where(Classes == target_cls)[0]\n    label = class_to_label[target_cls]\n\n    n_cols = 10 # # of sample plot\n\n    fig, axs = plt.subplots(ncols=n_cols, nrows=8, figsize=(25,9))\n\n    for i in range(n_cols):\n        \n        img = Images[indices[i]]\n        imgP = ImagePaths[indices[i]]\n        \n        # for cam\n        x = np.expand_dims(img, axis=0)\n        x = preprocess_input(copy.deepcopy(x))\n\n        # original\n        axs[0, i].imshow(np.uint8(img))\n        axs[0, i].set_title(label)\n        axs[0, i].set_xticks([])\n        axs[0, i].set_yticks([])\n        if i == 0:\n            axs[0, i].set_ylabel('Original', rotation=0, ha='right')\n\n        # Grad-CAM\n        cls_pred, cam = grad_cam(model=model, x=x, layer_name=layer_name)\n        _, _, img_grad_cam = superimpose(img, cam)\n        axs[1, i].imshow(img_grad_cam)\n        axs[1, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[1, i].set_xticks([])\n        axs[1, i].set_yticks([])\n        if i == 0:\n            axs[1, i].set_ylabel('Grad-CAM', rotation=0, ha='right')\n\n        # Grad-CAM++\n        cls_pred, cam = grad_cam_plus_plus(model=model, x=x, layer_name=layer_name)\n        _, _, img_grad_cam_plus_plus = superimpose(img, cam)\n        axs[2, i].imshow(img_grad_cam_plus_plus)\n        axs[2, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[2, i].set_xticks([])\n        axs[2, i].set_yticks([])\n        if i == 0:\n            axs[2, i].set_ylabel('Grad-CAM++', rotation=0, ha='right')\n\n        # Score-CAM\n        cls_pred, cam = score_cam(model=model, x=x, layer_name=layer_name)\n        _, _, img_score_cam = superimpose(img, cam)\n        axs[3, i].imshow(img_score_cam)\n        axs[3, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[3, i].set_xticks([])\n        axs[3, i].set_yticks([])\n        if i == 0:\n            axs[3, i].set_ylabel('Score-CAM', rotation=0, ha='right')\n        \n        # Extraction\n        img_ROI = extractROI(imgP, cam)\n        axs[4, i].imshow(img_ROI)\n        axs[4, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[4, i].set_xticks([])\n        axs[4, i].set_yticks([])\n        if i == 0:\n            axs[4, i].set_ylabel('ROI-CAM', rotation=0, ha='right')\n        \n        # Blood Vessels\n        imageBV = getBV(img_ROI)\n        axs[5, i].imshow(np.uint8(imageBV))\n        axs[5, i].set_title(label)\n        axs[5, i].set_xticks([])\n        axs[5, i].set_yticks([])\n        if i == 0:\n            axs[5, i].set_ylabel('BloodVessels', rotation=0, ha='right')\n            \n        # Exudates\n        imageE = getExudates(img_ROI)\n        axs[6, i].imshow(np.uint8(imageE))\n        axs[6, i].set_title(label)\n        axs[6, i].set_xticks([])\n        axs[6, i].set_yticks([])\n        if i == 0:\n            axs[6, i].set_ylabel('Exudates', rotation=0, ha='right')\n            \n        # Microaneurysms\n        imageMA = getMicroaneurysms(img_ROI)\n        axs[7, i].imshow(np.uint8(imageMA))\n        axs[7, i].set_title(label)\n        axs[7, i].set_xticks([])\n        axs[7, i].set_yticks([])\n        if i == 0:\n            axs[7, i].set_ylabel('Microaneurysms', rotation=0, ha='right')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T06:14:57.337375Z","iopub.execute_input":"2021-10-03T06:14:57.337936Z","iopub.status.idle":"2021-10-03T06:14:57.381334Z","shell.execute_reply.started":"2021-10-03T06:14:57.337663Z","shell.execute_reply":"2021-10-03T06:14:57.380488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## compare Grad-CAM / Grad-CAM++ / Score-CAM\n\ndef _compare(model, layer_name, target_cls):\n    \"\"\"compare Grad-CAM / Grad-CAM++ / Score-CAM on target class images\"\"\"\n    \n    indices = np.where(Classes == target_cls)[0]\n    random.shuffle(indices)\n    label = class_to_label[target_cls]\n\n    n_cols = 5 # # of sample plot\n\n    fig, axs = plt.subplots(ncols=n_cols, nrows=8, figsize=(50,50)) #(25, 9)\n    \n    idx = -1\n    i = -1\n    while n_cols != 0:\n        idx += 1\n        \n        if idx == len(indices):\n            break\n        if len(ImagePaths[indices[idx]][:-4]) != 12:\n            continue\n        \n        n_cols -= 1\n        i += 1\n        img = Images[indices[idx]]\n        imgP = os.path.join(\"../input/aptos2019-blindness-detection/train_images\", ImagePaths[indices[idx]])\n        \n        # for cam\n        x = np.expand_dims(img, axis=0)\n        x = preprocess_input(copy.deepcopy(x))\n\n        # original\n        axs[0, i].imshow(np.uint8(img))\n        axs[0, i].set_title(label)\n        axs[0, i].set_xticks([])\n        axs[0, i].set_yticks([])\n        if i == 0:\n            axs[0, i].set_ylabel('Original', rotation=0, ha='right')\n\n        # Grad-CAM\n        cls_pred, cam = grad_cam(model=model, x=x, layer_name=layer_name)\n        _, _, img_grad_cam = superimpose(img, cam)\n        axs[1, i].imshow(img_grad_cam)\n        axs[1, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[1, i].set_xticks([])\n        axs[1, i].set_yticks([])\n        if i == 0:\n            axs[1, i].set_ylabel('Grad-CAM', rotation=0, ha='right')\n\n        # Grad-CAM++\n        cls_pred, cam = grad_cam_plus_plus(model=model, x=x, layer_name=layer_name)\n        _, _, img_grad_cam_plus_plus = superimpose(img, cam)\n        axs[2, i].imshow(img_grad_cam_plus_plus)\n        axs[2, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[2, i].set_xticks([])\n        axs[2, i].set_yticks([])\n        if i == 0:\n            axs[2, i].set_ylabel('Grad-CAM++', rotation=0, ha='right')\n\n        # Score-CAM\n        cls_pred, cam = score_cam(model=model, x=x, layer_name=layer_name)\n        _, _, img_score_cam = superimpose(img, cam)\n        axs[3, i].imshow(img_score_cam)\n        axs[3, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[3, i].set_xticks([])\n        axs[3, i].set_yticks([])\n        if i == 0:\n            axs[3, i].set_ylabel('Score-CAM', rotation=0, ha='right')\n        \n        # Extraction\n        img_ROI = extractROI(imgP, cam)\n        axs[4, i].imshow(img_ROI)\n        axs[4, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[4, i].set_xticks([])\n        axs[4, i].set_yticks([])\n        if i == 0:\n            axs[4, i].set_ylabel('ROI-CAM', rotation=0, ha='right')\n        \n        img = cv2.imread(imgP)\n        img_ROI = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Blood Vessels\n        imageBV = getBV(img_ROI)\n        axs[5, i].imshow(np.uint8(imageBV))\n        axs[5, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[5, i].set_xticks([])\n        axs[5, i].set_yticks([])\n        if i == 0:\n            axs[5, i].set_ylabel('BloodVessels', rotation=0, ha='right')\n            \n        # Exudates\n        imageE = getExudates(img_ROI)\n        axs[6, i].imshow(np.uint8(imageE))\n        axs[6, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[6, i].set_xticks([])\n        axs[6, i].set_yticks([])\n        if i == 0:\n            axs[6, i].set_ylabel('Exudates', rotation=0, ha='right')\n            \n        # Microaneurysms\n        imageMA = getMicroaneurysms(img_ROI)\n        axs[7, i].imshow(np.uint8(imageMA))\n        axs[7, i].set_title('pred: ' + class_to_label[cls_pred])\n        axs[7, i].set_xticks([])\n        axs[7, i].set_yticks([])\n        if i == 0:\n            axs[7, i].set_ylabel('Microaneurysms', rotation=0, ha='right')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T06:16:37.962486Z","iopub.execute_input":"2021-10-03T06:16:37.962833Z","iopub.status.idle":"2021-10-03T06:16:37.996107Z","shell.execute_reply.started":"2021-10-03T06:16:37.962776Z","shell.execute_reply":"2021-10-03T06:16:37.995376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_compare(model=model, layer_name=model.layers[-2].name, target_cls=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T06:16:38.852696Z","iopub.execute_input":"2021-10-03T06:16:38.853187Z","iopub.status.idle":"2021-10-03T06:17:51.421752Z","shell.execute_reply.started":"2021-10-03T06:16:38.853081Z","shell.execute_reply":"2021-10-03T06:17:51.415204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_compare(model=model, layer_name=model.layers[-2].name, target_cls=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T06:20:06.217941Z","iopub.execute_input":"2021-10-03T06:20:06.218374Z","iopub.status.idle":"2021-10-03T06:21:17.467538Z","shell.execute_reply.started":"2021-10-03T06:20:06.218322Z","shell.execute_reply":"2021-10-03T06:21:17.464013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_compare(model=model, layer_name=model.layers[-2].name, target_cls=2)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T06:21:17.469497Z","iopub.execute_input":"2021-10-03T06:21:17.469986Z","iopub.status.idle":"2021-10-03T06:22:30.883305Z","shell.execute_reply.started":"2021-10-03T06:21:17.469936Z","shell.execute_reply":"2021-10-03T06:22:30.882243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_compare(model=model, layer_name=model.layers[-2].name, target_cls=3)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T06:22:30.885095Z","iopub.execute_input":"2021-10-03T06:22:30.885553Z","iopub.status.idle":"2021-10-03T06:23:47.913631Z","shell.execute_reply.started":"2021-10-03T06:22:30.885371Z","shell.execute_reply":"2021-10-03T06:23:47.910984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_compare(model=model, layer_name=model.layers[-2].name, target_cls=4)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T06:23:47.9158Z","iopub.execute_input":"2021-10-03T06:23:47.91631Z","iopub.status.idle":"2021-10-03T06:25:02.337064Z","shell.execute_reply.started":"2021-10-03T06:23:47.91626Z","shell.execute_reply":"2021-10-03T06:25:02.335925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}